{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Georgia's offensive play-calls under Jim Chaney using random forest classifier. \n",
    "This is directly based on the blog by Bill from https://collegefootballdata.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://api.collegefootballdata.com/teams/fbs\")\n",
    "teams = pd.read_json(response.text)\n",
    "\n",
    "teams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using CFBD API's /plays endpoint, loop through each of Jim Chaneys's years at Georgia (as OC/QB coach), starting in 2016, making sure to filter plays where Georgia is the offense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "\n",
    "for year in range(2016,2018):\n",
    "    response = requests.get(\"https://api.collegefootballdata.com/plays?seasonType=both&year={0}&offense=georgia\".format(year))\n",
    "    df = pd.io.json.json_normalize(response.json())\n",
    "    data = pd.concat([data, df])\n",
    "    \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleanup. We're only selecting variables that we think are relevant to play-calls and dropping the remaining fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['home', 'away', 'offense_score', 'defense_score', 'period', 'clock.minutes', \n",
    "             'clock.seconds', 'yardstogoal', 'down', 'distance', 'play_type']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new variable for home/away. We don't necessarily care about which team was home and which was away, but we do care whether the team calling the plays is at home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_home'] = np.where(data['home'] == 'Georgia', 1, 0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clock.minutes and clock.seconds fields are not really valuable independent of one another, so we convert them into a single field which is the raw seconds remaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['seconds_remaining'] = (data['clock.minutes'] * 60) + data['clock.seconds']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_types = ['Pass Reception', 'Pass Interception Return', 'Pass Incompletion', \n",
    "              'Sack', 'Passing Touchdown', 'Interception Return Touchdown']\n",
    "rush_types = ['Rush', 'Rushing Touchdown']\n",
    "punt_types = ['Punt', 'Punt Return Touchdown', 'Blocked Punt', 'Blocked Punt Touchdown']\n",
    "fg_types = ['Field Goal Good', 'Field Goal Missed', 'Blocked Field Goal']\n",
    "\n",
    "def getPlayCall(x):\n",
    "    if x in pass_types:\n",
    "            return 'pass'\n",
    "    elif x in rush_types:\n",
    "        return 'rush'\n",
    "    elif x in punt_types:\n",
    "        return 'punt'\n",
    "    elif x in fg_types:\n",
    "        return 'fg'\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "data['play_call'] = data['play_type'].apply(getPlayCall)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some play types don't fit into either of the four play call classifications (field goal, pass, punt, rush) and are set to `None`. We'll use teh convenient Pandas function to drop rows that have missing values, specifying which column or columns we want to be considered when looking for which rows to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(data['play_call']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['play_call'], inplace=True)\n",
    "print(pd.isna(data['play_call']).sum())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plays = data[['offense_score', 'defense_score', 'period', 'yardstogoal', \n",
    "              'down', 'distance', 'is_home', 'seconds_remaining', 'play_call']]\n",
    "plays.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a random forest prediction model\n",
    "We want our model to be able to take in a set of inputs regarding the game situation and use those inputs to predict play calls. Our dependent variable is the set of play calls, so everything else belongs in our feature set. \n",
    "1. Separate our set of features (our independent variables) from what we want our model to output (our dependent variable). \n",
    "2. Split our data into training and validation sets. Using the convenient `train_test_split` module we imported above, we are going to pull out 20% of the data to use as a validation set.\n",
    "    - We split out a validation set so that we can test out our model and ensure it is accurately predicting for the problem we are trying to solve, i.e. overfitting occurs when your model learns from the training set a little too good such that it's predictions are only good on the set on which it was trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data set between our independent variables (i.e. features) and our dependent variable or output\n",
    "play_calls = plays['play_call']\n",
    "plays = plays.drop(['play_call'], axis=1)\n",
    "\n",
    "# split the data into training and validation sets\n",
    "plays_train, plays_validation, calls_train, calls_validation = train_test_split(plays, play_calls, train_size=0.8, test_size=0.2, random_state=0)\n",
    "plays_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert our categorical data (dependent set of play calls) into numbers using pandas `factorize` method. This will return two sets:\n",
    "1. the data as a set of numbers ranging from 0 to 3\n",
    "2. the set containing the key mappings telling us which number mapped to which label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, y_keys = pd.factorize(calls_train)\n",
    "print(y[0:15,])\n",
    "print(y_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train a random forest classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the classifier\n",
    "classifier = RandomForestClassifier(random_state=0, n_estimators=100)\n",
    "\n",
    "# train the classifier with our test set\n",
    "classifier.fit(plays_train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass in our validation set of features to the `predict` method and see what the classifier outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predict(plays_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the `predict` method, which just outputs a single predicted value for each set of inputs, the `predict_proba` shown below provides a greater level of detail by outputting the probabilities for each set of inputs. Notice that we have four probabilities for each set of inputs which correspond to our four different output labels (pass/rush/fg/punt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predict_proba(plays_validation)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the raw outputs into labels using the y_keys mapping object we created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_calls = y_keys[classifier.predict(plays_validation)]\n",
    "predicted_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the predicted outputs with the actual outputs from our validation set. We can use the `crontab` functionality in pandas. Each row represents the actual classification of play calls in our validation set. The columns represent what our classifier predicted the play calls to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(calls_validation, predicted_calls, rownames=['Actual Calls'], colnames=['Predicted Calls'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving our model\n",
    "\n",
    "Evaluate our model predictions using the builtin `feature_importances_` property so we can see how it is weighting the importance of each feature in making its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(plays_train, classifier.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the `is_home` flag, as it's not helping the model and probably adding noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop is_home olumn\n",
    "plays = plays.drop(columns=['is_home'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same with the period field, our model isn't utilizing that flag. However, let's wrap that into seconds remaining instead of dropping it all together, similar to what we did with minutes earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorporate period into seconds_remaining\n",
    "plays['seconds_remaining'] = ((4 - plays['period']) * 15 * 60 ) + plays['seconds_remaining']\n",
    "\n",
    "# drop period column\n",
    "plays = plays.drop(columns=['period'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-run everything to see if our model improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plays_train, plays_validation, calls_train, calls_validation = train_test_split(plays, play_calls, train_size=0.8, test_size=0.2, random_state=0)\n",
    "y, y_keys = pd.factorize(calls_train)\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "classifier.fit(plays_train, y)\n",
    "\n",
    "predicted_calls = y_keys[classifier.predict(plays_validation)]\n",
    "\n",
    "pd.crosstab(calls_validation, predicted_calls, rownames=['Actual Calls'], colnames=['Predicted Calls'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOT IMPROVED***... let's look at the feature importance now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(plays_train, classifier.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offense and defense score flags are not really helping. Play calling would be more a function of how much a team is behind or ahead rather than the raw scores. Let's convert these two features into a single field, *score margin*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate new scoring margin field and drop the individual score columns\n",
    "plays['margin'] = plays['offense_score'] - plays['defense_score']\n",
    "plays = plays.drop(columns=['offense_score', 'defense_score'])\n",
    "\n",
    "plays_train, plays_validation, calls_train, calls_validation = train_test_split(plays, play_calls, train_size=0.8, test_size=0.2, random_state=0)\n",
    "y, y_keys = pd.factorize(calls_train)\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "classifier.fit(plays_train, y)\n",
    "\n",
    "predicted_calls = y_keys[classifier.predict(plays_validation)]\n",
    "\n",
    "pd.crosstab(calls_validation, predicted_calls, rownames=['Actual Calls'], colnames=['Predicted Calls'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***IMPROVED***... but not great"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate real-time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_call(yards, down, distance, seconds, margin):\n",
    "    test_plays = pd.DataFrame({'yardstogoal': [yards], 'down': [down], 'distance': [distance], 'seconds_remaining': [seconds], 'margin': [margin]})\n",
    "    return y_keys[classifier.predict(test_plays)][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say the ball is at the 50 yard line. It's 4th and 1 with 3 minutes left and Georgia is down by 3 points. What does Georgia do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call = predict_call(50, 4, 1, 180, -4)\n",
    "call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if Georgia is up by 10 points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call = predict_call(50, 4, 1, 180, 10)\n",
    "call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same prediction! What about being up by 50 points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call = predict_call(50, 4, 1, 180, 50)\n",
    "call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
